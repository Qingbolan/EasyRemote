# EasyRemote é«˜çº§åœºæ™¯æ•™ç¨‹

## ğŸ¯ å­¦ä¹ ç›®æ ‡

æœ¬æ•™ç¨‹å°†å¸¦æ‚¨æ·±å…¥EasyRemoteçš„é«˜çº§åº”ç”¨åœºæ™¯ï¼š
- æ„å»ºåˆ†å¸ƒå¼AIæ¨ç†æœåŠ¡
- å®ç°æ™ºèƒ½è´Ÿè½½å‡è¡¡ç­–ç•¥
- éƒ¨ç½²è¾¹ç¼˜è®¡ç®—ç½‘ç»œ
- å¤„ç†å®æ—¶æ•°æ®æµ
- æ„å»ºå®¹é”™å’Œé«˜å¯ç”¨ç³»ç»Ÿ

## ğŸ“‹ å‰ç½®è¦æ±‚

- å·²å®Œæˆ [åŸºç¡€ä½¿ç”¨æ•™ç¨‹](basic-usage.md)
- ç†Ÿæ‚‰Pythonå¼‚æ­¥ç¼–ç¨‹
- äº†è§£åŸºæœ¬çš„AI/MLæ¦‚å¿µ
- å…·å¤‡ç½‘ç»œç¼–ç¨‹ç»éªŒ

## ğŸ¤– åœºæ™¯ä¸€ï¼šåˆ†å¸ƒå¼AIæ¨ç†æœåŠ¡

### ç›®æ ‡
æ„å»ºä¸€ä¸ªæ”¯æŒå¤šæ¨¡å‹ã€å¤šèŠ‚ç‚¹çš„AIæ¨ç†æœåŠ¡ï¼Œå®ç°è´Ÿè½½åˆ†æ‹…å’Œæ¨¡å‹çƒ­åˆ‡æ¢ã€‚

### æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client Apps   â”‚    â”‚   Web Dashboard â”‚    â”‚  Mobile Apps    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Gateway Server       â”‚
                    â”‚    (Load Balancer)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                     â”‚                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  GPU Node 1   â”‚    â”‚   GPU Node 2    â”‚    â”‚ CPU Node 3  â”‚
    â”‚  (BERT Model) â”‚    â”‚ (Image Classifierâ”‚    â”‚ (Text Proc) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®ç°æ­¥éª¤

#### 1. åˆ›å»ºæ¨¡å‹æŠ½è±¡åŸºç±»

```python
# ai_models/base_model.py
from abc import ABC, abstractmethod
import time
import logging

class BaseAIModel(ABC):
    def __init__(self, model_name: str, model_path: str = None):
        self.model_name = model_name
        self.model_path = model_path
        self.model = None
        self.load_time = None
        self.inference_count = 0
        self.logger = logging.getLogger(f"AIModel.{model_name}")
        
    @abstractmethod
    def load_model(self):
        """åŠ è½½æ¨¡å‹åˆ°å†…å­˜"""
        pass
    
    @abstractmethod
    def preprocess(self, raw_input):
        """é¢„å¤„ç†è¾“å…¥æ•°æ®"""
        pass
    
    @abstractmethod
    def inference(self, processed_input):
        """æ‰§è¡Œæ¨¡å‹æ¨ç†"""
        pass
    
    @abstractmethod
    def postprocess(self, raw_output):
        """åå¤„ç†è¾“å‡ºç»“æœ"""
        pass
    
    def predict(self, input_data):
        """å®Œæ•´çš„é¢„æµ‹æµç¨‹"""
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†
            processed_input = self.preprocess(input_data)
            
            # æ¨ç†
            raw_output = self.inference(processed_input)
            
            # åå¤„ç†
            result = self.postprocess(raw_output)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.inference_count += 1
            inference_time = time.time() - start_time
            
            self.logger.info(f"æ¨ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.3f}s")
            
            return {
                "result": result,
                "model_name": self.model_name,
                "inference_time": inference_time,
                "confidence": getattr(self, '_last_confidence', None)
            }
            
        except Exception as e:
            self.logger.error(f"æ¨ç†å¤±è´¥: {e}")
            raise
    
    def get_stats(self):
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "inference_count": self.inference_count,
            "load_time": self.load_time,
            "model_loaded": self.model is not None
        }
```

#### 2. å®ç°å…·ä½“AIæ¨¡å‹

```python
# ai_models/bert_model.py
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from .base_model import BaseAIModel

class BertSentimentModel(BaseAIModel):
    def __init__(self):
        super().__init__("bert-sentiment", "bert-base-uncased")
        self.tokenizer = None
        self.max_length = 512
        
    def load_model(self):
        """åŠ è½½BERTæ¨¡å‹"""
        start_time = time.time()
        
        self.logger.info("å¼€å§‹åŠ è½½BERTæ¨¡å‹...")
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        self.tokenizer = BertTokenizer.from_pretrained(self.model_path)
        self.model = BertForSequenceClassification.from_pretrained(
            self.model_path,
            num_labels=3  # positive, negative, neutral
        )
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        self.load_time = time.time() - start_time
        self.logger.info(f"BERTæ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {self.load_time:.2f}s")
        
    def preprocess(self, text_input):
        """é¢„å¤„ç†æ–‡æœ¬è¾“å…¥"""
        if isinstance(text_input, str):
            texts = [text_input]
        else:
            texts = text_input
            
        # åˆ†è¯å’Œç¼–ç 
        encoded = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return encoded
    
    def inference(self, encoded_input):
        """æ‰§è¡ŒBERTæ¨ç†"""
        with torch.no_grad():
            outputs = self.model(**encoded_input)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            
        return predictions
    
    def postprocess(self, predictions):
        """åå¤„ç†æ¨ç†ç»“æœ"""
        # è·å–é¢„æµ‹æ ‡ç­¾å’Œç½®ä¿¡åº¦
        predicted_classes = torch.argmax(predictions, dim=-1)
        confidences = torch.max(predictions, dim=-1).values
        
        # æ ‡ç­¾æ˜ å°„
        label_map = {0: "negative", 1: "neutral", 2: "positive"}
        
        results = []
        for i, (pred_class, confidence) in enumerate(zip(predicted_classes, confidences)):
            results.append({
                "label": label_map[pred_class.item()],
                "confidence": confidence.item()
            })
            
        # ä¿å­˜æœ€åçš„ç½®ä¿¡åº¦ç”¨äºç»Ÿè®¡
        self._last_confidence = confidences.mean().item()
        
        return results[0] if len(results) == 1 else results

# ai_models/image_classifier.py  
import torch
import torchvision.transforms as transforms
from PIL import Image
import io
import base64
from .base_model import BaseAIModel

class ImageClassificationModel(BaseAIModel):
    def __init__(self):
        super().__init__("resnet-imagenet", "resnet50")
        self.transform = None
        self.class_names = None
        
    def load_model(self):
        """åŠ è½½å›¾åƒåˆ†ç±»æ¨¡å‹"""
        start_time = time.time()
        
        self.logger.info("å¼€å§‹åŠ è½½ResNetæ¨¡å‹...")
        
        # åŠ è½½é¢„è®­ç»ƒçš„ResNetæ¨¡å‹
        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)
        self.model.eval()
        
        # å®šä¹‰å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225]),
        ])
        
        # åŠ è½½ImageNetç±»åˆ«åç§°
        self._load_imagenet_classes()
        
        self.load_time = time.time() - start_time
        self.logger.info(f"ResNetæ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {self.load_time:.2f}s")
        
    def _load_imagenet_classes(self):
        """åŠ è½½ImageNetç±»åˆ«åç§°"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä»æ–‡ä»¶åŠ è½½
        self.class_names = [f"class_{i}" for i in range(1000)]
    
    def preprocess(self, image_input):
        """é¢„å¤„ç†å›¾åƒè¾“å…¥"""
        if isinstance(image_input, str):
            # å‡è®¾æ˜¯base64ç¼–ç çš„å›¾åƒ
            image_data = base64.b64decode(image_input)
            image = Image.open(io.BytesIO(image_data))
        elif isinstance(image_input, bytes):
            image = Image.open(io.BytesIO(image_input))
        else:
            raise ValueError("ä¸æ”¯æŒçš„å›¾åƒè¾“å…¥æ ¼å¼")
            
        # è½¬æ¢ä¸ºRGB
        if image.mode != 'RGB':
            image = image.convert('RGB')
            
        # åº”ç”¨å˜æ¢
        tensor = self.transform(image).unsqueeze(0)
        return tensor
    
    def inference(self, image_tensor):
        """æ‰§è¡Œå›¾åƒåˆ†ç±»æ¨ç†"""
        with torch.no_grad():
            outputs = self.model(image_tensor)
            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
            
        return probabilities
    
    def postprocess(self, probabilities):
        """åå¤„ç†åˆ†ç±»ç»“æœ"""
        # è·å–top-5é¢„æµ‹
        top5_prob, top5_indices = torch.topk(probabilities, 5)
        
        results = []
        for i, (prob, idx) in enumerate(zip(top5_prob, top5_indices)):
            results.append({
                "rank": i + 1,
                "class_id": idx.item(),
                "class_name": self.class_names[idx.item()],
                "probability": prob.item()
            })
            
        self._last_confidence = top5_prob[0].item()
        return results
```

#### 3. åˆ›å»ºAIèŠ‚ç‚¹ç®¡ç†å™¨

```python
# ai_nodes/ai_node_manager.py
from easyremote import ComputeNode
import threading
import queue
import time
from ai_models.bert_model import BertSentimentModel
from ai_models.image_classifier import ImageClassificationModel

class AINodeManager:
    def __init__(self, server_address, node_type="general"):
        self.node = ComputeNode(server_address)
        self.node_type = node_type
        self.models = {}
        self.model_queue = queue.Queue()
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0
        }
        
    def load_models(self):
        """æ ¹æ®èŠ‚ç‚¹ç±»å‹åŠ è½½ç›¸åº”æ¨¡å‹"""
        if self.node_type in ["nlp", "general"]:
            self.logger.info("åŠ è½½NLPæ¨¡å‹...")
            bert_model = BertSentimentModel()
            bert_model.load_model()
            self.models["bert_sentiment"] = bert_model
            
        if self.node_type in ["vision", "general"]:
            self.logger.info("åŠ è½½è§†è§‰æ¨¡å‹...")
            image_model = ImageClassificationModel()
            image_model.load_model()
            self.models["image_classification"] = image_model
            
    def register_ai_functions(self):
        """æ³¨å†ŒAIæ¨ç†å‡½æ•°"""
        
        @self.node.register
        def text_sentiment_analysis(text):
            """æ–‡æœ¬æƒ…æ„Ÿåˆ†æ"""
            if "bert_sentiment" not in self.models:
                raise RuntimeError("BERTæ¨¡å‹æœªåŠ è½½")
                
            start_time = time.time()
            try:
                result = self.models["bert_sentiment"].predict(text)
                self._update_stats(True, time.time() - start_time)
                return result
            except Exception as e:
                self._update_stats(False, time.time() - start_time)
                raise
        
        @self.node.register
        def image_classification(image_data):
            """å›¾åƒåˆ†ç±»"""
            if "image_classification" not in self.models:
                raise RuntimeError("å›¾åƒåˆ†ç±»æ¨¡å‹æœªåŠ è½½")
                
            start_time = time.time()
            try:
                result = self.models["image_classification"].predict(image_data)
                self._update_stats(True, time.time() - start_time)
                return result
            except Exception as e:
                self._update_stats(False, time.time() - start_time)
                raise
        
        @self.node.register
        def batch_text_analysis(texts):
            """æ‰¹é‡æ–‡æœ¬åˆ†æ"""
            if "bert_sentiment" not in self.models:
                raise RuntimeError("BERTæ¨¡å‹æœªåŠ è½½")
                
            results = []
            for text in texts:
                try:
                    result = self.models["bert_sentiment"].predict(text)
                    results.append(result)
                except Exception as e:
                    results.append({"error": str(e)})
                    
            return results
        
        @self.node.register
        def get_node_status():
            """è·å–èŠ‚ç‚¹çŠ¶æ€"""
            model_stats = {}
            for name, model in self.models.items():
                model_stats[name] = model.get_stats()
                
            return {
                "node_type": self.node_type,
                "loaded_models": list(self.models.keys()),
                "model_stats": model_stats,
                "request_stats": self.stats
            }
            
    def _update_stats(self, success, response_time):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["total_requests"] += 1
        if success:
            self.stats["successful_requests"] += 1
        else:
            self.stats["failed_requests"] += 1
            
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total_time = self.stats["average_response_time"] * (self.stats["total_requests"] - 1)
        self.stats["average_response_time"] = (total_time + response_time) / self.stats["total_requests"]
    
    def start_serving(self):
        """å¼€å§‹æä¾›æœåŠ¡"""
        self.load_models()
        self.register_ai_functions()
        
        print(f"ğŸ¤– AIèŠ‚ç‚¹å¯åŠ¨ - ç±»å‹: {self.node_type}")
        print(f"ğŸ“š å·²åŠ è½½æ¨¡å‹: {list(self.models.keys())}")
        
        self.node.serve()
```

#### 4. åˆ›å»ºæ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨

```python
# load_balancer/smart_balancer.py
import time
import random
from collections import defaultdict
from typing import Dict, List, Optional

class SmartLoadBalancer:
    def __init__(self):
        self.nodes = {}
        self.node_stats = defaultdict(dict)
        self.model_to_nodes = defaultdict(list)
        self.strategies = {
            "round_robin": self._round_robin,
            "least_connections": self._least_connections,
            "resource_aware": self._resource_aware,
            "model_affinity": self._model_affinity
        }
        
    def register_node(self, node_id: str, node_info: dict):
        """æ³¨å†Œè®¡ç®—èŠ‚ç‚¹"""
        self.nodes[node_id] = {
            "info": node_info,
            "last_seen": time.time(),
            "active_requests": 0,
            "total_requests": 0,
            "average_response_time": 0
        }
        
        # æ›´æ–°æ¨¡å‹åˆ°èŠ‚ç‚¹çš„æ˜ å°„
        for model in node_info.get("models", []):
            if node_id not in self.model_to_nodes[model]:
                self.model_to_nodes[model].append(node_id)
    
    def select_node(self, function_name: str, strategy: str = "resource_aware") -> Optional[str]:
        """é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹"""
        if strategy not in self.strategies:
            strategy = "resource_aware"
            
        return self.strategies[strategy](function_name)
    
    def _round_robin(self, function_name: str) -> Optional[str]:
        """è½®è¯¢ç­–ç•¥"""
        available_nodes = self._get_available_nodes(function_name)
        if not available_nodes:
            return None
            
        # ç®€å•è½®è¯¢
        return available_nodes[int(time.time()) % len(available_nodes)]
    
    def _least_connections(self, function_name: str) -> Optional[str]:
        """æœ€å°‘è¿æ¥ç­–ç•¥"""
        available_nodes = self._get_available_nodes(function_name)
        if not available_nodes:
            return None
            
        # é€‰æ‹©æ´»è·ƒè¯·æ±‚æ•°æœ€å°‘çš„èŠ‚ç‚¹
        min_connections = float('inf')
        best_node = None
        
        for node_id in available_nodes:
            connections = self.nodes[node_id]["active_requests"]
            if connections < min_connections:
                min_connections = connections
                best_node = node_id
                
        return best_node
    
    def _resource_aware(self, function_name: str) -> Optional[str]:
        """èµ„æºæ„ŸçŸ¥ç­–ç•¥"""
        available_nodes = self._get_available_nodes(function_name)
        if not available_nodes:
            return None
            
        # ç»¼åˆè€ƒè™‘CPUã€å†…å­˜ã€å“åº”æ—¶é—´ç­‰å› ç´ 
        best_score = float('inf')
        best_node = None
        
        for node_id in available_nodes:
            node = self.nodes[node_id]
            stats = self.node_stats[node_id]
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            cpu_score = stats.get("cpu_usage", 50) / 100
            memory_score = stats.get("memory_usage", 50) / 100
            response_score = node["average_response_time"] / 10  # å‡è®¾10sä¸ºåŸºå‡†
            load_score = node["active_requests"] / 10  # å‡è®¾10ä¸ºåŸºå‡†
            
            total_score = cpu_score + memory_score + response_score + load_score
            
            if total_score < best_score:
                best_score = total_score
                best_node = node_id
                
        return best_node
    
    def _model_affinity(self, function_name: str) -> Optional[str]:
        """æ¨¡å‹äº²å’Œæ€§ç­–ç•¥"""
        # æ ¹æ®å‡½æ•°åæ¨æ–­æ‰€éœ€æ¨¡å‹
        model_map = {
            "text_sentiment_analysis": "bert_sentiment",
            "image_classification": "image_classification",
            "batch_text_analysis": "bert_sentiment"
        }
        
        required_model = model_map.get(function_name)
        if required_model and required_model in self.model_to_nodes:
            candidate_nodes = self.model_to_nodes[required_model]
            available_nodes = [n for n in candidate_nodes if self._is_node_available(n)]
            
            if available_nodes:
                # åœ¨æœ‰ç›¸å…³æ¨¡å‹çš„èŠ‚ç‚¹ä¸­é€‰æ‹©è´Ÿè½½æœ€ä½çš„
                return self._least_connections_from_nodes(available_nodes)
        
        # å›é€€åˆ°èµ„æºæ„ŸçŸ¥ç­–ç•¥
        return self._resource_aware(function_name)
    
    def _get_available_nodes(self, function_name: str) -> List[str]:
        """è·å–å¯ç”¨èŠ‚ç‚¹åˆ—è¡¨"""
        return [node_id for node_id in self.nodes 
                if self._is_node_available(node_id)]
    
    def _is_node_available(self, node_id: str) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¯ç”¨"""
        if node_id not in self.nodes:
            return False
            
        node = self.nodes[node_id]
        # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦åœ¨æœ€è¿‘30ç§’å†…æ´»è·ƒ
        return time.time() - node["last_seen"] < 30
    
    def _least_connections_from_nodes(self, nodes: List[str]) -> Optional[str]:
        """ä»æŒ‡å®šèŠ‚ç‚¹ä¸­é€‰æ‹©è¿æ¥æ•°æœ€å°‘çš„"""
        if not nodes:
            return None
            
        min_connections = float('inf')
        best_node = None
        
        for node_id in nodes:
            connections = self.nodes[node_id]["active_requests"]
            if connections < min_connections:
                min_connections = connections
                best_node = node_id
                
        return best_node
    
    def update_node_stats(self, node_id: str, stats: dict):
        """æ›´æ–°èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯"""
        if node_id in self.nodes:
            self.nodes[node_id]["last_seen"] = time.time()
            self.node_stats[node_id].update(stats)
    
    def start_request(self, node_id: str):
        """æ ‡è®°è¯·æ±‚å¼€å§‹"""
        if node_id in self.nodes:
            self.nodes[node_id]["active_requests"] += 1
            self.nodes[node_id]["total_requests"] += 1
    
    def finish_request(self, node_id: str, response_time: float):
        """æ ‡è®°è¯·æ±‚å®Œæˆ"""
        if node_id in self.nodes:
            node = self.nodes[node_id]
            node["active_requests"] = max(0, node["active_requests"] - 1)
            
            # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
            total_time = node["average_response_time"] * (node["total_requests"] - 1)
            node["average_response_time"] = (total_time + response_time) / node["total_requests"]
```

#### 5. æ™ºèƒ½ç½‘å…³æœåŠ¡å™¨

```python
# gateway/smart_gateway.py
from easyremote import Server
import asyncio
import time
import logging
from load_balancer.smart_balancer import SmartLoadBalancer

class SmartGatewayServer:
    def __init__(self, host="0.0.0.0", port=8080):
        self.server = Server(host=host, port=port)
        self.load_balancer = SmartLoadBalancer()
        self.logger = logging.getLogger("SmartGateway")
        
    def setup_advanced_routing(self):
        """è®¾ç½®é«˜çº§è·¯ç”±é€»è¾‘"""
        
        @self.server.register_middleware
        def intelligent_routing(request):
            """æ™ºèƒ½è·¯ç”±ä¸­é—´ä»¶"""
            function_name = request.function_name
            
            # é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
            selected_node = self.load_balancer.select_node(
                function_name, 
                strategy="model_affinity"
            )
            
            if not selected_node:
                raise RuntimeError(f"æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹å¤„ç†å‡½æ•°: {function_name}")
            
            # è®°å½•è¯·æ±‚å¼€å§‹
            self.load_balancer.start_request(selected_node)
            request.target_node = selected_node
            
            return request
        
        @self.server.register_middleware
        def request_monitoring(request):
            """è¯·æ±‚ç›‘æ§ä¸­é—´ä»¶"""
            start_time = time.time()
            
            try:
                # æ‰§è¡Œè¯·æ±‚
                response = yield
                
                # è®°å½•æˆåŠŸ
                response_time = time.time() - start_time
                self.load_balancer.finish_request(
                    request.target_node, 
                    response_time
                )
                
                self.logger.info(
                    f"è¯·æ±‚æˆåŠŸ - å‡½æ•°: {request.function_name}, "
                    f"èŠ‚ç‚¹: {request.target_node}, "
                    f"è€—æ—¶: {response_time:.3f}s"
                )
                
                return response
                
            except Exception as e:
                # è®°å½•å¤±è´¥
                response_time = time.time() - start_time
                self.load_balancer.finish_request(
                    request.target_node, 
                    response_time
                )
                
                self.logger.error(
                    f"è¯·æ±‚å¤±è´¥ - å‡½æ•°: {request.function_name}, "
                    f"èŠ‚ç‚¹: {request.target_node}, "
                    f"é”™è¯¯: {e}"
                )
                
                raise
        
    async def start_health_monitor(self):
        """å¯åŠ¨å¥åº·ç›‘æ§"""
        while True:
            try:
                # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„å¥åº·çŠ¶æ€
                for node_id in list(self.load_balancer.nodes.keys()):
                    try:
                        # è¿™é‡Œåº”è¯¥è°ƒç”¨èŠ‚ç‚¹çš„å¥åº·æ£€æŸ¥å‡½æ•°
                        # stats = await self.call_node_function(node_id, "get_node_status")
                        # self.load_balancer.update_node_stats(node_id, stats)
                        pass
                    except Exception as e:
                        self.logger.warning(f"èŠ‚ç‚¹ {node_id} å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                
                await asyncio.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"å¥åº·ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(5)
    
    def start(self):
        """å¯åŠ¨æ™ºèƒ½ç½‘å…³"""
        self.setup_advanced_routing()
        
        # å¯åŠ¨å¥åº·ç›‘æ§
        asyncio.create_task(self.start_health_monitor())
        
        self.logger.info("ğŸš€ æ™ºèƒ½ç½‘å…³å¯åŠ¨ï¼Œæ”¯æŒä»¥ä¸‹åŠŸèƒ½:")
        self.logger.info("  - æ™ºèƒ½è´Ÿè½½å‡è¡¡")
        self.logger.info("  - æ¨¡å‹äº²å’Œæ€§è·¯ç”±")
        self.logger.info("  - å®æ—¶å¥åº·ç›‘æ§")
        self.logger.info("  - è¯·æ±‚é“¾è·¯è¿½è¸ª")
        
        self.server.start()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    gateway = SmartGatewayServer()
    gateway.start()
```

## ğŸŒ åœºæ™¯äºŒï¼šè¾¹ç¼˜è®¡ç®—ç½‘ç»œ

### ç›®æ ‡
æ„å»ºä¸€ä¸ªåˆ†å¸ƒå¼è¾¹ç¼˜è®¡ç®—ç½‘ç»œï¼Œæ”¯æŒå°±è¿‘è®¡ç®—ã€æ•°æ®æœ¬åœ°åŒ–å’Œæ™ºèƒ½åè°ƒã€‚

### å®ç°

```python
# edge_computing/edge_device.py
import json
import time
import psutil
import asyncio
from datetime import datetime
from easyremote import ComputeNode, Client

class EdgeDevice:
    def __init__(self, device_id, location, gateway_address):
        self.device_id = device_id
        self.location = location
        self.node = ComputeNode(gateway_address)
        self.client = Client(gateway_address)
        self.local_cache = {}
        self.device_stats = {}
        
    def register_edge_functions(self):
        """æ³¨å†Œè¾¹ç¼˜è®¡ç®—å‡½æ•°"""
        
        @self.node.register
        def process_sensor_data(sensor_readings):
            """å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®"""
            timestamp = datetime.now().isoformat()
            
            # æœ¬åœ°æ•°æ®å¤„ç†
            processed_data = {
                "device_id": self.device_id,
                "location": self.location,
                "timestamp": timestamp,
                "raw_readings": sensor_readings,
                "processed_readings": {
                    "average": sum(sensor_readings) / len(sensor_readings),
                    "max": max(sensor_readings),
                    "min": min(sensor_readings),
                    "variance": self._calculate_variance(sensor_readings)
                },
                "anomaly_detected": self._detect_anomaly(sensor_readings)
            }
            
            # ç¼“å­˜å¤„ç†ç»“æœ
            self.local_cache[timestamp] = processed_data
            
            return processed_data
        
        @self.node.register
        def edge_analytics(data_points, analysis_type="trend"):
            """è¾¹ç¼˜æ•°æ®åˆ†æ"""
            if analysis_type == "trend":
                return self._trend_analysis(data_points)
            elif analysis_type == "correlation":
                return self._correlation_analysis(data_points)
            elif analysis_type == "prediction":
                return self._simple_prediction(data_points)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}")
        
        @self.node.register
        def coordinate_with_neighbors(task_data):
            """ä¸é‚»è¿‘è®¾å¤‡åè°ƒ"""
            nearby_devices = self._find_nearby_devices()
            
            results = []
            for device in nearby_devices:
                try:
                    # å§”æ‰˜éƒ¨åˆ†ä»»åŠ¡ç»™é‚»è¿‘è®¾å¤‡
                    result = self.client.execute(
                        f"edge_task_delegation_{device}",
                        task_data
                    )
                    results.append({
                        "device": device,
                        "result": result,
                        "success": True
                    })
                except Exception as e:
                    results.append({
                        "device": device,
                        "error": str(e),
                        "success": False
                    })
            
            return {
                "coordinator": self.device_id,
                "task_results": results,
                "coordination_time": datetime.now().isoformat()
            }
        
        @self.node.register
        def get_device_metrics():
            """è·å–è®¾å¤‡æŒ‡æ ‡"""
            return {
                "device_id": self.device_id,
                "location": self.location,
                "system_metrics": {
                    "cpu_percent": psutil.cpu_percent(),
                    "memory_percent": psutil.virtual_memory().percent,
                    "disk_usage": psutil.disk_usage('/').percent,
                    "network_io": psutil.net_io_counters()._asdict(),
                    "boot_time": psutil.boot_time()
                },
                "cache_stats": {
                    "cached_items": len(self.local_cache),
                    "cache_size_mb": self._get_cache_size()
                },
                "timestamp": datetime.now().isoformat()
            }
    
    def _calculate_variance(self, values):
        """è®¡ç®—æ–¹å·®"""
        mean = sum(values) / len(values)
        return sum((x - mean) ** 2 for x in values) / len(values)
    
    def _detect_anomaly(self, values):
        """ç®€å•å¼‚å¸¸æ£€æµ‹"""
        if len(values) < 2:
            return False
            
        mean = sum(values) / len(values)
        variance = self._calculate_variance(values)
        threshold = 2 * (variance ** 0.5)  # 2å€æ ‡å‡†å·®
        
        return any(abs(x - mean) > threshold for x in values)
    
    def _trend_analysis(self, data_points):
        """è¶‹åŠ¿åˆ†æ"""
        if len(data_points) < 2:
            return {"trend": "insufficient_data"}
        
        # ç®€å•çº¿æ€§è¶‹åŠ¿
        x_values = list(range(len(data_points)))
        y_values = data_points
        
        n = len(data_points)
        sum_x = sum(x_values)
        sum_y = sum(y_values)
        sum_xy = sum(x * y for x, y in zip(x_values, y_values))
        sum_x2 = sum(x * x for x in x_values)
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        
        return {
            "trend": "increasing" if slope > 0 else "decreasing" if slope < 0 else "stable",
            "slope": slope,
            "confidence": abs(slope) * 100  # ç®€åŒ–çš„ç½®ä¿¡åº¦
        }
    
    def _correlation_analysis(self, data_points):
        """ç›¸å…³æ€§åˆ†æ"""
        # è¿™é‡Œç®€åŒ–ä¸ºä¸æ—¶é—´çš„ç›¸å…³æ€§
        if len(data_points) < 3:
            return {"correlation": "insufficient_data"}
        
        x_values = list(range(len(data_points)))
        correlation = self._pearson_correlation(x_values, data_points)
        
        return {
            "time_correlation": correlation,
            "interpretation": self._interpret_correlation(correlation)
        }
    
    def _simple_prediction(self, data_points):
        """ç®€å•é¢„æµ‹"""
        if len(data_points) < 3:
            return {"prediction": "insufficient_data"}
        
        # ä½¿ç”¨ç®€å•ç§»åŠ¨å¹³å‡
        window_size = min(3, len(data_points))
        recent_average = sum(data_points[-window_size:]) / window_size
        
        # è®¡ç®—è¶‹åŠ¿
        trend = self._trend_analysis(data_points)
        
        # ç®€å•é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼
        if trend["trend"] == "increasing":
            prediction = recent_average * 1.1
        elif trend["trend"] == "decreasing":
            prediction = recent_average * 0.9
        else:
            prediction = recent_average
        
        return {
            "predicted_value": prediction,
            "confidence": min(90, len(data_points) * 10),  # ç®€åŒ–çš„ç½®ä¿¡åº¦
            "method": "moving_average_with_trend"
        }
    
    def _pearson_correlation(self, x, y):
        """è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°"""
        n = len(x)
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(xi * yi for xi, yi in zip(x, y))
        sum_x2 = sum(xi * xi for xi in x)
        sum_y2 = sum(yi * yi for yi in y)
        
        numerator = n * sum_xy - sum_x * sum_y
        denominator = ((n * sum_x2 - sum_x * sum_x) * (n * sum_y2 - sum_y * sum_y)) ** 0.5
        
        return numerator / denominator if denominator != 0 else 0
    
    def _interpret_correlation(self, correlation):
        """è§£é‡Šç›¸å…³ç³»æ•°"""
        abs_corr = abs(correlation)
        if abs_corr > 0.8:
            return "strong"
        elif abs_corr > 0.5:
            return "moderate"
        elif abs_corr > 0.3:
            return "weak"
        else:
            return "very_weak"
    
    def _find_nearby_devices(self):
        """æŸ¥æ‰¾é‚»è¿‘è®¾å¤‡"""
        # ç®€åŒ–çš„é‚»è¿‘è®¾å¤‡å‘ç°
        all_devices = ["edge_001", "edge_002", "edge_003", "edge_004"]
        return [d for d in all_devices if d != self.device_id][:2]  # æœ€å¤š2ä¸ªé‚»è¿‘è®¾å¤‡
    
    def _get_cache_size(self):
        """è·å–ç¼“å­˜å¤§å°ï¼ˆMBï¼‰"""
        total_size = 0
        for value in self.local_cache.values():
            total_size += len(str(value))
        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    
    async def start_data_collection(self):
        """å¯åŠ¨æ•°æ®æ”¶é›†"""
        while True:
            try:
                # æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®æ”¶é›†
                sensor_data = [
                    random.uniform(20, 30),  # æ¸©åº¦
                    random.uniform(40, 60),  # æ¹¿åº¦
                    random.uniform(990, 1020)  # æ°”å‹
                ]
                
                # å¤„ç†æ•°æ®
                result = await self.process_sensor_data_async(sensor_data)
                
                # å¦‚æœæ£€æµ‹åˆ°å¼‚å¸¸ï¼Œé€šçŸ¥å…¶ä»–è®¾å¤‡
                if result.get("anomaly_detected"):
                    await self.notify_anomaly(result)
                
                await asyncio.sleep(10)  # æ¯10ç§’æ”¶é›†ä¸€æ¬¡æ•°æ®
                
            except Exception as e:
                print(f"æ•°æ®æ”¶é›†é”™è¯¯: {e}")
                await asyncio.sleep(5)
    
    async def process_sensor_data_async(self, sensor_data):
        """å¼‚æ­¥å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å¤„ç†å‡½æ•°
        return {
            "device_id": self.device_id,
            "data": sensor_data,
            "timestamp": datetime.now().isoformat()
        }
    
    async def notify_anomaly(self, anomaly_data):
        """é€šçŸ¥å¼‚å¸¸æƒ…å†µ"""
        print(f"ğŸš¨ æ£€æµ‹åˆ°å¼‚å¸¸: {anomaly_data}")
        
        # é€šçŸ¥é‚»è¿‘è®¾å¤‡
        nearby_devices = self._find_nearby_devices()
        for device in nearby_devices:
            try:
                await self.client.execute_async(
                    f"receive_anomaly_alert_{device}",
                    anomaly_data
                )
            except Exception as e:
                print(f"é€šçŸ¥è®¾å¤‡ {device} å¤±è´¥: {e}")
    
    def start_serving(self):
        """å¼€å§‹æä¾›è¾¹ç¼˜è®¡ç®—æœåŠ¡"""
        self.register_edge_functions()
        
        print(f"ğŸŒ è¾¹ç¼˜è®¾å¤‡å¯åŠ¨")
        print(f"ğŸ“ è®¾å¤‡ID: {self.device_id}")
        print(f"ğŸ“ ä½ç½®: {self.location}")
        print(f"ğŸ”§ æä¾›æœåŠ¡:")
        print(f"  - ä¼ æ„Ÿå™¨æ•°æ®å¤„ç†")
        print(f"  - æœ¬åœ°æ•°æ®åˆ†æ")
        print(f"  - è®¾å¤‡åè°ƒ")
        print(f"  - ç³»ç»Ÿç›‘æ§")
        
        # å¯åŠ¨æ•°æ®æ”¶é›†
        asyncio.create_task(self.start_data_collection())
        
        self.node.serve()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import random
    
    # åˆ›å»ºè¾¹ç¼˜è®¾å¤‡
    devices = [
        EdgeDevice("edge_beijing_001", "Beijing, China", "gateway:8080"),
        EdgeDevice("edge_shanghai_002", "Shanghai, China", "gateway:8080"),
        EdgeDevice("edge_shenzhen_003", "Shenzhen, China", "gateway:8080")
    ]
    
    # å¯åŠ¨ç¬¬ä¸€ä¸ªè®¾å¤‡ï¼ˆåœ¨å®é™…ç¯å¢ƒä¸­ï¼Œæ¯ä¸ªè®¾å¤‡ç‹¬ç«‹è¿è¡Œï¼‰
    devices[0].start_serving()
```

## ğŸ“Š åœºæ™¯ä¸‰ï¼šå®æ—¶æµå¤„ç†ç³»ç»Ÿ

### ç›®æ ‡
æ„å»ºä¸€ä¸ªæ”¯æŒå®æ—¶æ•°æ®æµå¤„ç†çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†é«˜å¹¶å‘æ•°æ®æµå¹¶æä¾›å®æ—¶åˆ†æã€‚

### å®ç°

```python
# streaming/stream_processor.py
import asyncio
import queue
import time
import json
from collections import deque, defaultdict
from easyremote import ComputeNode, Client

class StreamProcessor:
    def __init__(self, processor_id, gateway_address):
        self.processor_id = processor_id
        self.node = ComputeNode(gateway_address)
        self.client = Client(gateway_address)
        
        # æµå¤„ç†ç»„ä»¶
        self.input_queue = asyncio.Queue(maxsize=1000)
        self.output_queue = asyncio.Queue(maxsize=1000)
        self.processing_stats = defaultdict(int)
        
        # çª—å£å¤„ç†
        self.time_windows = defaultdict(deque)
        self.window_size = 60  # 60ç§’çª—å£
        
    def register_stream_functions(self):
        """æ³¨å†Œæµå¤„ç†å‡½æ•°"""
        
        @self.node.register
        def process_stream_data(data_batch):
            """å¤„ç†æµæ•°æ®æ‰¹æ¬¡"""
            results = []
            
            for item in data_batch:
                try:
                    processed_item = self._process_single_item(item)
                    results.append(processed_item)
                    self.processing_stats["processed"] += 1
                except Exception as e:
                    results.append({"error": str(e), "original": item})
                    self.processing_stats["errors"] += 1
            
            return {
                "processor_id": self.processor_id,
                "batch_size": len(data_batch),
                "results": results,
                "processing_time": time.time()
            }
        
        @self.node.register
        def window_aggregation(window_type="time", window_size=60):
            """çª—å£èšåˆ"""
            current_time = time.time()
            
            if window_type == "time":
                # æ—¶é—´çª—å£èšåˆ
                window_data = self._get_time_window_data(current_time, window_size)
                return self._aggregate_window_data(window_data)
            
            elif window_type == "count":
                # è®¡æ•°çª—å£èšåˆ
                window_data = self._get_count_window_data(window_size)
                return self._aggregate_window_data(window_data)
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„çª—å£ç±»å‹: {window_type}")
        
        @self.node.register
        def real_time_analytics(data_stream, metric_type="average"):
            """å®æ—¶åˆ†æ"""
            if metric_type == "average":
                return self._calculate_moving_average(data_stream)
            elif metric_type == "trend":
                return self._detect_trend(data_stream)
            elif metric_type == "anomaly":
                return self._detect_stream_anomaly(data_stream)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {metric_type}")
        
        @self.node.register
        def stream_join(stream_a, stream_b, join_key):
            """æµè¿æ¥æ“ä½œ"""
            return self._join_streams(stream_a, stream_b, join_key)
        
        @self.node.register
        def get_stream_stats():
            """è·å–æµå¤„ç†ç»Ÿè®¡"""
            return {
                "processor_id": self.processor_id,
                "stats": dict(self.processing_stats),
                "queue_sizes": {
                    "input": self.input_queue.qsize(),
                    "output": self.output_queue.qsize()
                },
                "window_stats": {
                    key: len(window) 
                    for key, window in self.time_windows.items()
                }
            }
    
    def _process_single_item(self, item):
        """å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
        # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
        processed_item = {
            "original": item,
            "processed_time": time.time(),
            "processor_id": self.processor_id
        }
        
        # æ ¹æ®æ•°æ®ç±»å‹è¿›è¡Œä¸åŒå¤„ç†
        if isinstance(item, dict):
            processed_item.update(self._process_dict_item(item))
        elif isinstance(item, (int, float)):
            processed_item.update(self._process_numeric_item(item))
        elif isinstance(item, str):
            processed_item.update(self._process_text_item(item))
        else:
            processed_item["type"] = "unknown"
        
        # æ·»åŠ åˆ°æ—¶é—´çª—å£
        window_key = int(time.time() // self.window_size)
        self.time_windows[window_key].append(processed_item)
        
        # æ¸…ç†æ—§çª—å£
        self._cleanup_old_windows()
        
        return processed_item
    
    def _process_dict_item(self, item):
        """å¤„ç†å­—å…¸ç±»å‹æ•°æ®"""
        return {
            "type": "dict",
            "keys": list(item.keys()),
            "size": len(item),
            "has_timestamp": "timestamp" in item,
            "summary": self._summarize_dict(item)
        }
    
    def _process_numeric_item(self, item):
        """å¤„ç†æ•°å€¼ç±»å‹æ•°æ®"""
        return {
            "type": "numeric",
            "value": item,
            "is_integer": isinstance(item, int),
            "absolute_value": abs(item),
            "normalized": self._normalize_value(item)
        }
    
    def _process_text_item(self, item):
        """å¤„ç†æ–‡æœ¬ç±»å‹æ•°æ®"""
        words = item.split()
        return {
            "type": "text",
            "length": len(item),
            "word_count": len(words),
            "has_numbers": any(char.isdigit() for char in item),
            "uppercase_ratio": sum(1 for c in item if c.isupper()) / len(item) if item else 0
        }
    
    def _summarize_dict(self, item):
        """å­—å…¸æ•°æ®æ‘˜è¦"""
        summary = {}
        for key, value in item.items():
            if isinstance(value, (int, float)):
                summary[f"{key}_type"] = "numeric"
                summary[f"{key}_value"] = value
            elif isinstance(value, str):
                summary[f"{key}_type"] = "text"
                summary[f"{key}_length"] = len(value)
            else:
                summary[f"{key}_type"] = type(value).__name__
        return summary
    
    def _normalize_value(self, value):
        """å½’ä¸€åŒ–æ•°å€¼"""
        # ç®€å•çš„å½’ä¸€åŒ–ï¼Œå®é™…åº”è¯¥åŸºäºå†å²æ•°æ®
        return max(-1, min(1, value / 100))
    
    def _get_time_window_data(self, current_time, window_size):
        """è·å–æ—¶é—´çª—å£æ•°æ®"""
        target_window = int((current_time - window_size) // self.window_size)
        window_data = []
        
        for window_key, items in self.time_windows.items():
            if window_key >= target_window:
                window_data.extend(items)
        
        return window_data
    
    def _get_count_window_data(self, count):
        """è·å–è®¡æ•°çª—å£æ•°æ®"""
        all_items = []
        for items in self.time_windows.values():
            all_items.extend(items)
        
        return all_items[-count:] if len(all_items) >= count else all_items
    
    def _aggregate_window_data(self, window_data):
        """èšåˆçª—å£æ•°æ®"""
        if not window_data:
            return {"count": 0, "message": "no_data"}
        
        # åŸºç¡€ç»Ÿè®¡
        count = len(window_data)
        
        # æ•°å€¼ç»Ÿè®¡
        numeric_values = []
        for item in window_data:
            if item.get("type") == "numeric":
                numeric_values.append(item["value"])
        
        aggregation = {
            "count": count,
            "numeric_count": len(numeric_values),
            "window_start": min(item["processed_time"] for item in window_data),
            "window_end": max(item["processed_time"] for item in window_data)
        }
        
        if numeric_values:
            aggregation.update({
                "sum": sum(numeric_values),
                "average": sum(numeric_values) / len(numeric_values),
                "min": min(numeric_values),
                "max": max(numeric_values),
                "median": sorted(numeric_values)[len(numeric_values) // 2]
            })
        
        # ç±»å‹åˆ†å¸ƒ
        type_counts = defaultdict(int)
        for item in window_data:
            type_counts[item.get("type", "unknown")] += 1
        
        aggregation["type_distribution"] = dict(type_counts)
        
        return aggregation
    
    def _calculate_moving_average(self, data_stream, window=10):
        """è®¡ç®—ç§»åŠ¨å¹³å‡"""
        if len(data_stream) < window:
            window = len(data_stream)
        
        if window == 0:
            return {"average": 0, "count": 0}
        
        recent_values = data_stream[-window:]
        numeric_values = [x for x in recent_values if isinstance(x, (int, float))]
        
        if not numeric_values:
            return {"average": 0, "count": 0, "message": "no_numeric_data"}
        
        return {
            "average": sum(numeric_values) / len(numeric_values),
            "count": len(numeric_values),
            "window_size": window,
            "min": min(numeric_values),
            "max": max(numeric_values)
        }
    
    def _detect_trend(self, data_stream):
        """æ£€æµ‹è¶‹åŠ¿"""
        if len(data_stream) < 3:
            return {"trend": "insufficient_data"}
        
        # å–æœ€è¿‘çš„æ•°å€¼æ•°æ®
        numeric_values = [x for x in data_stream[-10:] if isinstance(x, (int, float))]
        
        if len(numeric_values) < 3:
            return {"trend": "insufficient_numeric_data"}
        
        # ç®€å•è¶‹åŠ¿æ£€æµ‹
        differences = [numeric_values[i+1] - numeric_values[i] 
                      for i in range(len(numeric_values)-1)]
        
        positive_changes = sum(1 for d in differences if d > 0)
        negative_changes = sum(1 for d in differences if d < 0)
        
        if positive_changes > negative_changes:
            trend = "increasing"
        elif negative_changes > positive_changes:
            trend = "decreasing"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "confidence": abs(positive_changes - negative_changes) / len(differences),
            "change_points": len(differences),
            "average_change": sum(differences) / len(differences)
        }
    
    def _detect_stream_anomaly(self, data_stream):
        """æ£€æµ‹æµå¼‚å¸¸"""
        if len(data_stream) < 5:
            return {"anomaly": False, "reason": "insufficient_data"}
        
        numeric_values = [x for x in data_stream if isinstance(x, (int, float))]
        
        if len(numeric_values) < 5:
            return {"anomaly": False, "reason": "insufficient_numeric_data"}
        
        # ä½¿ç”¨ç®€å•çš„ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å¼‚å¸¸
        mean = sum(numeric_values) / len(numeric_values)
        variance = sum((x - mean) ** 2 for x in numeric_values) / len(numeric_values)
        std_dev = variance ** 0.5
        
        # æ£€æµ‹æœ€æ–°å€¼æ˜¯å¦å¼‚å¸¸
        latest_value = numeric_values[-1]
        z_score = abs(latest_value - mean) / std_dev if std_dev > 0 else 0
        
        is_anomaly = z_score > 2  # 2å€æ ‡å‡†å·®
        
        return {
            "anomaly": is_anomaly,
            "z_score": z_score,
            "latest_value": latest_value,
            "mean": mean,
            "std_dev": std_dev,
            "threshold": 2
        }
    
    def _join_streams(self, stream_a, stream_b, join_key):
        """è¿æ¥ä¸¤ä¸ªæµ"""
        result = []
        
        # åˆ›å»ºè¿æ¥ç´¢å¼•
        index_b = {}
        for item in stream_b:
            if isinstance(item, dict) and join_key in item:
                key_value = item[join_key]
                if key_value not in index_b:
                    index_b[key_value] = []
                index_b[key_value].append(item)
        
        # æ‰§è¡Œè¿æ¥
        for item_a in stream_a:
            if isinstance(item_a, dict) and join_key in item_a:
                key_value = item_a[join_key]
                if key_value in index_b:
                    for item_b in index_b[key_value]:
                        joined_item = {
                            "stream_a": item_a,
                            "stream_b": item_b,
                            "join_key": join_key,
                            "join_value": key_value
                        }
                        result.append(joined_item)
        
        return {
            "joined_count": len(result),
            "stream_a_size": len(stream_a),
            "stream_b_size": len(stream_b),
            "join_key": join_key,
            "results": result
        }
    
    def _cleanup_old_windows(self):
        """æ¸…ç†æ—§çš„æ—¶é—´çª—å£"""
        current_time = time.time()
        cutoff_window = int((current_time - self.window_size * 5) // self.window_size)
        
        # åˆ é™¤è¶…è¿‡5ä¸ªçª—å£æœŸçš„æ—§æ•°æ®
        old_windows = [key for key in self.time_windows.keys() if key < cutoff_window]
        for key in old_windows:
            del self.time_windows[key]
    
    async def stream_worker(self):
        """æµå¤„ç†å·¥ä½œè€…"""
        while True:
            try:
                # æ£€æŸ¥è¾“å…¥é˜Ÿåˆ—
                if not self.input_queue.empty():
                    # æ‰¹é‡å¤„ç†
                    batch = []
                    batch_size = min(10, self.input_queue.qsize())
                    
                    for _ in range(batch_size):
                        try:
                            item = await asyncio.wait_for(
                                self.input_queue.get(), 
                                timeout=0.1
                            )
                            batch.append(item)
                        except asyncio.TimeoutError:
                            break
                    
                    if batch:
                        # å¤„ç†æ‰¹æ¬¡
                        result = await self.process_batch_async(batch)
                        await self.output_queue.put(result)
                
                await asyncio.sleep(0.01)  # é˜²æ­¢è¿‡åº¦å ç”¨CPU
                
            except Exception as e:
                print(f"æµå¤„ç†å·¥ä½œè€…é”™è¯¯: {e}")
                await asyncio.sleep(1)
    
    async def process_batch_async(self, batch):
        """å¼‚æ­¥å¤„ç†æ‰¹æ¬¡"""
        # è¿™é‡Œå¯ä»¥è°ƒç”¨å®é™…çš„æµå¤„ç†å‡½æ•°
        return {
            "batch_id": int(time.time() * 1000),
            "processed_items": len(batch),
            "processor_id": self.processor_id,
            "timestamp": time.time()
        }
    
    def start_serving(self):
        """å¼€å§‹æä¾›æµå¤„ç†æœåŠ¡"""
        self.register_stream_functions()
        
        print(f"ğŸŒŠ æµå¤„ç†å™¨å¯åŠ¨")
        print(f"ğŸ†” å¤„ç†å™¨ID: {self.processor_id}")
        print(f"ğŸ”§ æä¾›æœåŠ¡:")
        print(f"  - å®æ—¶æ•°æ®æµå¤„ç†")
        print(f"  - çª—å£èšåˆåˆ†æ")
        print(f"  - æµè¿æ¥æ“ä½œ")
        print(f"  - å¼‚å¸¸æ£€æµ‹")
        
        # å¯åŠ¨æµå¤„ç†å·¥ä½œè€…
        asyncio.create_task(self.stream_worker())
        
        self.node.serve()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    processor = StreamProcessor("stream_proc_001", "gateway:8080")
    processor.start_serving()
```

## ğŸ”— é›†æˆä¸éƒ¨ç½²

### å®Œæ•´éƒ¨ç½²è„šæœ¬

```python
# deploy/deploy_advanced_system.py
import asyncio
import subprocess
import time
import json
from pathlib import Path

class AdvancedSystemDeployer:
    def __init__(self, config_file="deployment_config.json"):
        self.config = self._load_config(config_file)
        self.processes = []
        
    def _load_config(self, config_file):
        """åŠ è½½éƒ¨ç½²é…ç½®"""
        default_config = {
            "gateway": {
                "host": "0.0.0.0",
                "port": 8080,
                "enable_monitoring": True
            },
            "ai_nodes": [
                {"type": "nlp", "count": 2},
                {"type": "vision", "count": 1},
                {"type": "general", "count": 1}
            ],
            "edge_devices": [
                {"id": "edge_001", "location": "Beijing"},
                {"id": "edge_002", "location": "Shanghai"},
                {"id": "edge_003", "location": "Shenzhen"}
            ],
            "stream_processors": [
                {"id": "stream_001", "type": "analytics"},
                {"id": "stream_002", "type": "aggregation"}
            ]
        }
        
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
        except FileNotFoundError:
            config = default_config
            # ä¿å­˜é»˜è®¤é…ç½®
            with open(config_file, 'w') as f:
                json.dump(default_config, f, indent=2)
        
        return config
    
    def deploy_gateway(self):
        """éƒ¨ç½²æ™ºèƒ½ç½‘å…³"""
        print("ğŸš€ éƒ¨ç½²æ™ºèƒ½ç½‘å…³...")
        
        gateway_script = f"""
from gateway.smart_gateway import SmartGatewayServer
import logging

logging.basicConfig(level=logging.INFO)
gateway = SmartGatewayServer(
    host="{self.config['gateway']['host']}", 
    port={self.config['gateway']['port']}
)
gateway.start()
"""
        
        # å¯åŠ¨ç½‘å…³è¿›ç¨‹
        process = subprocess.Popen([
            "python", "-c", gateway_script
        ])
        self.processes.append(("gateway", process))
        
        # ç­‰å¾…ç½‘å…³å¯åŠ¨
        time.sleep(3)
        print("âœ… æ™ºèƒ½ç½‘å…³éƒ¨ç½²å®Œæˆ")
    
    def deploy_ai_nodes(self):
        """éƒ¨ç½²AIè®¡ç®—èŠ‚ç‚¹"""
        print("ğŸ¤– éƒ¨ç½²AIè®¡ç®—èŠ‚ç‚¹...")
        
        server_address = f"{self.config['gateway']['host']}:{self.config['gateway']['port']}"
        
        for node_config in self.config['ai_nodes']:
            node_type = node_config['type']
            count = node_config['count']
            
            for i in range(count):
                node_script = f"""
from ai_nodes.ai_node_manager import AINodeManager
import logging

logging.basicConfig(level=logging.INFO)
node = AINodeManager("{server_address}", "{node_type}")
node.start_serving()
"""
                
                process = subprocess.Popen([
                    "python", "-c", node_script
                ])
                self.processes.append((f"ai_node_{node_type}_{i}", process))
                
                time.sleep(2)  # é¿å…åŒæ—¶å¯åŠ¨è¿‡å¤šè¿›ç¨‹
        
        print("âœ… AIè®¡ç®—èŠ‚ç‚¹éƒ¨ç½²å®Œæˆ")
    
    def deploy_edge_devices(self):
        """éƒ¨ç½²è¾¹ç¼˜è®¾å¤‡"""
        print("ğŸŒ éƒ¨ç½²è¾¹ç¼˜è®¡ç®—è®¾å¤‡...")
        
        server_address = f"{self.config['gateway']['host']}:{self.config['gateway']['port']}"
        
        for device_config in self.config['edge_devices']:
            device_id = device_config['id']
            location = device_config['location']
            
            device_script = f"""
from edge_computing.edge_device import EdgeDevice
import logging

logging.basicConfig(level=logging.INFO)
device = EdgeDevice("{device_id}", "{location}", "{server_address}")
device.start_serving()
"""
            
            process = subprocess.Popen([
                "python", "-c", device_script
            ])
            self.processes.append((f"edge_{device_id}", process))
            
            time.sleep(1)
        
        print("âœ… è¾¹ç¼˜è®¡ç®—è®¾å¤‡éƒ¨ç½²å®Œæˆ")
    
    def deploy_stream_processors(self):
        """éƒ¨ç½²æµå¤„ç†å™¨"""
        print("ğŸŒŠ éƒ¨ç½²æµå¤„ç†å™¨...")
        
        server_address = f"{self.config['gateway']['host']}:{self.config['gateway']['port']}"
        
        for proc_config in self.config['stream_processors']:
            proc_id = proc_config['id']
            proc_type = proc_config['type']
            
            proc_script = f"""
from streaming.stream_processor import StreamProcessor
import logging

logging.basicConfig(level=logging.INFO)
processor = StreamProcessor("{proc_id}", "{server_address}")
processor.start_serving()
"""
            
            process = subprocess.Popen([
                "python", "-c", proc_script
            ])
            self.processes.append((f"stream_{proc_id}", process))
            
            time.sleep(1)
        
        print("âœ… æµå¤„ç†å™¨éƒ¨ç½²å®Œæˆ")
    
    def deploy_all(self):
        """éƒ¨ç½²å®Œæ•´ç³»ç»Ÿ"""
        print("ğŸš€ å¼€å§‹éƒ¨ç½²EasyRemoteé«˜çº§åˆ†å¸ƒå¼ç³»ç»Ÿ...")
        print("=" * 60)
        
        try:
            # æŒ‰é¡ºåºéƒ¨ç½²å„ç»„ä»¶
            self.deploy_gateway()
            self.deploy_ai_nodes()
            self.deploy_edge_devices()
            self.deploy_stream_processors()
            
            print("\n" + "=" * 60)
            print("ğŸ‰ ç³»ç»Ÿéƒ¨ç½²å®Œæˆï¼")
            print(f"ğŸ“Š å·²å¯åŠ¨ {len(self.processes)} ä¸ªç»„ä»¶")
            print(f"ğŸŒ ç½‘å…³åœ°å€: {self.config['gateway']['host']}:{self.config['gateway']['port']}")
            
            # æ˜¾ç¤ºç»„ä»¶çŠ¶æ€
            self.show_system_status()
            
        except Exception as e:
            print(f"âŒ éƒ¨ç½²å¤±è´¥: {e}")
            self.cleanup()
    
    def show_system_status(self):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        print("\nğŸ“‹ ç³»ç»Ÿç»„ä»¶çŠ¶æ€:")
        print("-" * 40)
        
        for name, process in self.processes:
            status = "è¿è¡Œä¸­" if process.poll() is None else "å·²åœæ­¢"
            print(f"  {name}: {status}")
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰è¿›ç¨‹"""
        print("\nğŸ§¹ æ¸…ç†ç³»ç»Ÿè¿›ç¨‹...")
        
        for name, process in self.processes:
            try:
                process.terminate()
                process.wait(timeout=5)
                print(f"âœ… å·²åœæ­¢: {name}")
            except subprocess.TimeoutExpired:
                process.kill()
                print(f"ğŸ”ª å¼ºåˆ¶åœæ­¢: {name}")
            except Exception as e:
                print(f"âŒ åœæ­¢å¤±è´¥ {name}: {e}")
        
        self.processes.clear()
        print("âœ… æ¸…ç†å®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    deployer = AdvancedSystemDeployer()
    
    try:
        deployer.deploy_all()
        
        # ä¿æŒè¿è¡Œ
        print("\næŒ‰ Ctrl+C åœæ­¢ç³»ç»Ÿ...")
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\næ”¶åˆ°åœæ­¢ä¿¡å·...")
        deployer.cleanup()
    except Exception as e:
        print(f"ç³»ç»Ÿé”™è¯¯: {e}")
        deployer.cleanup()
```

## ğŸ æ€»ç»“

é€šè¿‡æœ¬é«˜çº§æ•™ç¨‹ï¼Œæ‚¨å·²ç»å­¦ä¼šäº†ï¼š

1. **ğŸ¤– åˆ†å¸ƒå¼AIæ¨ç†æœåŠ¡**: æ„å»ºæ”¯æŒå¤šæ¨¡å‹çš„æ™ºèƒ½è®¡ç®—ç½‘ç»œ
2. **âš–ï¸ æ™ºèƒ½è´Ÿè½½å‡è¡¡**: å®ç°èµ„æºæ„ŸçŸ¥å’Œæ¨¡å‹äº²å’Œæ€§è·¯ç”±
3. **ğŸŒ è¾¹ç¼˜è®¡ç®—ç½‘ç»œ**: éƒ¨ç½²åˆ†å¸ƒå¼è¾¹ç¼˜è®¾å¤‡åä½œç³»ç»Ÿ
4. **ğŸŒŠ å®æ—¶æµå¤„ç†**: å¤„ç†é«˜å¹¶å‘æ•°æ®æµå’Œå®æ—¶åˆ†æ
5. **ğŸš€ ç³»ç»Ÿé›†æˆéƒ¨ç½²**: å®Œæ•´çš„åˆ†å¸ƒå¼ç³»ç»Ÿéƒ¨ç½²æ–¹æ¡ˆ

è¿™äº›é«˜çº§åœºæ™¯å±•ç¤ºäº†EasyRemoteåœ¨å¤æ‚åˆ†å¸ƒå¼è®¡ç®—ç¯å¢ƒä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚æ‚¨å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©å’Œç»„åˆè¿™äº›ç»„ä»¶ï¼Œæ„å»ºé€‚åˆçš„åˆ†å¸ƒå¼è®¡ç®—è§£å†³æ–¹æ¡ˆã€‚

## ğŸ”— ç›¸å…³èµ„æº

- ğŸ“– [åŸºç¡€ä½¿ç”¨æ•™ç¨‹](basic-usage.md)
- ğŸ“š [APIå‚è€ƒæ–‡æ¡£](../user-guide/api-reference.md)
- ğŸ—ï¸ [æ¶æ„è®¾è®¡æ–‡æ¡£](../architecture/overview.md)
- ğŸ’¡ [ç¤ºä¾‹ä»£ç åº“](../user-guide/examples.md) 